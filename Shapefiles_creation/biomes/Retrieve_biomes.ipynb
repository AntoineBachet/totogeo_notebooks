{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:34.789582Z",
     "start_time": "2019-04-16T12:57:33.010982Z"
    }
   },
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages = [\"harsha2010:magellan:1.0.5-s_2.11\"]\n",
    "launcher.conf.set(\"spark.ui.port\", \"4041\")\n",
    "launcher.conf.set(\"spark.ui.reverseProxyUrl\", \"https://gisui.totosearch.org\")\n",
    "launcher.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "launcher.conf.set(\"spark.driver.memory\", \"8g\")\n",
    "launcher.conf.set(\"spark.executor.cores\", \"2\")\n",
    "launcher.conf.set(\"spark.driver.maxResultSize\", \"10G\")\n",
    "launcher.conf.set(\"spark.executor.pyspark.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:37.569989Z",
     "start_time": "2019-04-16T12:57:34.791234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.17.0.2:4041\n",
       "SparkContext available as 'sc' (version = 2.2.3, master = local[*], app id = local-1555419454566)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SQLContext\n",
       "sqlCtx: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@52311b62\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlCtx = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:38.075071Z",
     "start_time": "2019-04-16T12:57:37.571643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import magellan.{Point, Polygon}\n",
       "import org.apache.spark.sql.magellan.dsl.expressions._\n",
       "import org.apache.spark.sql.types._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import magellan.{Point, Polygon}\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:42.094886Z",
     "start_time": "2019-04-16T12:57:38.076749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             polygon|            metadata|\n",
      "+--------------------+--------------------+\n",
      "|magellan.Polygon@...|Map(xcoord -> 157...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 157...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 157...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -12...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -12...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 127...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 127...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 127...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -92...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -92...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -92...|\n",
      "|magellan.Polygon@...|Map(xcoord -> 26....|\n",
      "|magellan.Polygon@...|Map(xcoord -> 26....|\n",
      "|magellan.Polygon@...|Map(xcoord -> 82....|\n",
      "|magellan.Polygon@...|Map(xcoord -> 82....|\n",
      "|magellan.Polygon@...|Map(xcoord -> 70....|\n",
      "|magellan.Polygon@...|Map(xcoord -> -12...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -12...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -12...|\n",
      "|magellan.Polygon@...|Map(xcoord -> -65...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [point: point, polyline: polyline ... 3 more fields]\n",
       "polygons: org.apache.spark.sql.DataFrame = [polygon: polygon, metadata: map<string,string>]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sqlCtx.read.format(\"magellan\").load(\"/tf/Team 6 - Project/Finding_the_Biomes/Biomes/\")\n",
    "val polygons = df.select(\"polygon\",\"metadata\")\n",
    "\n",
    "polygons.show()\n",
    "//df.select(\"metadata\").take(100).last(0).asInstanceOf[Map[String, String]](\"BIOME_NUM\").trim().toDouble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:42.846663Z",
     "start_time": "2019-04-16T12:57:42.096416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               point|\n",
      "+--------------------+\n",
      "|Point(62.09999999...|\n",
      "|Point(2.333333, 4...|\n",
      "|    Point(1.0, -1.0)|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "points: org.apache.spark.sql.DataFrame = [point: point]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val points = sc.parallelize(Seq((62.09999999998624,-88.70000000000006), (2.333333, 48.866667), (1.0, -1.0))).toDF(\"x\", \"y\").select(point($\"x\", $\"y\").as(\"point\"))\n",
    "\n",
    "points.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:54.824280Z",
     "start_time": "2019-04-16T12:57:42.847818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------+----------+\n",
      "|        Longitude|          Latitude|          BIOME_NAME|BIOME_NUM|BIOME_CODE|\n",
      "+-----------------+------------------+--------------------+---------+----------+\n",
      "|62.09999999998624|-88.70000000000006|                 N/A|     11.0|       N/A|\n",
      "|         2.333333|         48.866667|Temperate Broadle...|      4.0|      PA04|\n",
      "+-----------------+------------------+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "points.join(polygons).where($\"point\" within $\"polygon\").\n",
    "collect().\n",
    "map(x => (x(0).asInstanceOf[magellan.Point].getX(), \n",
    "          x(0).asInstanceOf[magellan.Point].getY(),\n",
    "          x(2).asInstanceOf[Map[String, String]](\"BIOME_NAME\").trim().toString,\n",
    "         x(2).asInstanceOf[Map[String, String]](\"BIOME_NUM\").trim().toDouble,\n",
    "        x(2).asInstanceOf[Map[String, String]](\"ECO_BIOME_\").trim().toString)).\n",
    "toSeq.\n",
    "toDF(\"Longitude\", \"Latitude\", \"BIOME_NAME\", \"BIOME_NUM\", \"BIOME_CODE\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T12:57:55.643512Z",
     "start_time": "2019-04-16T12:57:54.825911Z"
    }
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Path does not exist: file:/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_21411;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Path does not exist: file:/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_21411;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:626)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.foreach(List.scala:381)",
      "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.flatMap(List.scala:344)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)",
      "  at $anonfun$1.apply$mcVI$sp(<console>:56)",
      "  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)",
      "  ... 43 elided",
      ""
     ]
    }
   ],
   "source": [
    "def extractDouble(expectedNumber: Any): Option[Double] = expectedNumber match {\n",
    "    case d: Double => Some(d)\n",
    "    case i: Int => Some(i.toDouble)\n",
    "    case l: Long => Some(l.toDouble)\n",
    "    case s: String => scala.util.Try(s.trim.toDouble).toOption\n",
    "    case _ => None\n",
    "  } \n",
    "\n",
    "//21411\n",
    "for (i <- 21411 to 21411){\n",
    "    var df_points = spark.read.\n",
    "    format(\"csv\").\n",
    "    option(\"header\", \"true\").\n",
    "    load(s\"/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_$i\")\n",
    "    \n",
    "    //df_points.show()\n",
    "    var grid = df_points.collect()\n",
    "    \n",
    "    var points_grid = sc.parallelize(grid.map(x => (extractDouble(x(0)),extractDouble(x(1)))).collect{case (Some(i), Some(j)) => (i,j)}.toSeq).toDF(\"Longitude\", \"Latitude\").select(point($\"Longitude\", $\"Latitude\").as(\"point\"))\n",
    "    \n",
    "    points_grid.join(polygons).where($\"point\" within $\"polygon\").\n",
    "    collect().\n",
    "    map(x => (x(0).asInstanceOf[magellan.Point].getX(), \n",
    "              x(0).asInstanceOf[magellan.Point].getY(),\n",
    "              x(2).asInstanceOf[Map[String, String]](\"BIOME_NAME\").trim().toString,\n",
    "             x(2).asInstanceOf[Map[String, String]](\"BIOME_NUM\").trim().toDouble,\n",
    "            x(2).asInstanceOf[Map[String, String]](\"ECO_BIOME_\").trim().toString)).\n",
    "    toSeq.\n",
    "    toDF(\"Longitude\", \"Latitude\", \"BIOME_NAME\", \"BIOME_NUM\", \"BIOME_CODE\").\n",
    "    write.format(\"csv\").option(\"header\", \"true\").save(s\"/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_$i/grid\")\n",
    "\n",
    "\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T23:27:40.312Z"
    }
   },
   "outputs": [],
   "source": [
    "//%%python\n",
    "//import pandas as pd\n",
    "//data = pd.read_csv(\"/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/grid_simple-Copy1.csv\")\n",
    "\n",
    "//import os\n",
    "//for i in range(21410):\n",
    "//    os.mkdir('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_'+str(i+1))\n",
    "//    (data.iloc[i*100:(i+1)*100,:]).to_csv('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_'+str(i+1)+'/Grid_'+str(i+1)+'.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.17.0.2:4040\n",
       "SparkContext available as 'sc' (version = 2.2.3, master = local[*], app id = local-1554750825484)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "(2, 'No such file or directory')",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 3, in <module>",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_359/grid'"
     ]
    }
   ],
   "source": [
    "//%%python\n",
    "//import os\n",
    "//for i in range(359,360):\n",
    "//    files = os.listdir('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_'+str(i)+'/grid')\n",
    "//    for file in files:\n",
    "//        os.remove('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_'+str(i)+'/grid/' + file)\n",
    "//        print(\"File\", file, \"removed\")\n",
    "//    os.rmdir('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_'+str(i)+'/grid/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21410.54\n"
     ]
    }
   ],
   "source": [
    "//%%python\n",
    "//import os\n",
    "\n",
    "//import pandas as pd\n",
    "//data = pd.read_csv(\"/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/grid_simple-Copy1.csv\")\n",
    "\n",
    "//files = os.listdir('/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/')\n",
    "//print(len(data)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+----------+---------+----------+\n",
      "|          Longitude|         Latitude|BIOME_NAME|BIOME_NUM|BIOME_CODE|\n",
      "+-------------------+-----------------+----------+---------+----------+\n",
      "| -112.9000000000038|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.80000000000382|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.70000000000385|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.60000000000386|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.50000000000385|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.40000000000386|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.30000000000385|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.20000000000385|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-112.10000000000386|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "| -107.9000000000041|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "| -107.8000000000041|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.70000000000412|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.60000000000412|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.50000000000412|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.40000000000413|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.30000000000412|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.20000000000414|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-107.10000000000414|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-115.40000000000369|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "|-115.30000000000368|-88.5000000000001|       N/A|     11.0|       N/A|\n",
      "+-------------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "i: Int = 1\n",
       "df_points: org.apache.spark.sql.DataFrame = [Longitude: string, Latitude: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val i = 1\n",
    "//var df_points = spark.read.\n",
    "//    format(\"csv\").\n",
    "//    option(\"header\", \"true\").\n",
    "//    load(s\"/tf/Team 6 - Project/Creating shapefiles/Grid shapefiles/Grid_Simple/partial/Grid_$i/grid/\")\n",
    "    \n",
    "//df_points.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
